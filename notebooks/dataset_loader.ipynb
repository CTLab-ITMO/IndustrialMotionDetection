{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'IndustrialMotionDetection'...\n",
      "remote: Enumerating objects: 327, done.\u001b[K\n",
      "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
      "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
      "remote: Total 327 (delta 58), reused 128 (delta 46), pack-reused 172 (from 1)\u001b[K\n",
      "Receiving objects: 100% (327/327), 49.39 MiB | 35.20 MiB/s, done.\n",
      "Resolving deltas: 100% (114/114), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone -b ruslan-dev https://github.com/CTLab-ITMO/IndustrialMotionDetection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/IndustrialMotionDetection/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd IndustrialMotionDetection/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path=['/kaggle/working/IndustrialMotionDetection/src', '/kaggle/working', '/kaggle/lib/kagglegym', '/kaggle/lib', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorchvideo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d460cc1d5dee>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{sys.path=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomDatasetDecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYamlConfigReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/IndustrialMotionDetection/src/models/dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorchvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoded_video\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncodedVideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_list\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoxList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorchvideo'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def add_path(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "add_path(str(Path().cwd().parent.absolute() / \"src\"))\n",
    "print(f\"{sys.path=}\")\n",
    "\n",
    "from models.dataset import RandomDatasetDecord\n",
    "from config import YamlConfigReader\n",
    "from models.dataset import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ruslan/src/IndustrialMotionDetection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruslan/src/IndustrialMotionDetection/.venv/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ruslan/src/IndustrialMotionDetection'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file_path = 'conf/meva_preproc.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotations_csv': 'data/MEVA/meva_processed/annotations.csv',\n",
       " 'train': 'data/MEVA/meva_processed/train.csv',\n",
       " 'test': 'data/MEVA/meva_processed/test.csv',\n",
       " 'annotations_folder': 'data/MEVA/meva-data-repo/annotation/DIVA-phase-2/MEVA/kitware-meva-training',\n",
       " 'bbox_area_limit': 10000,\n",
       " 'display_annotations': False,\n",
       " 'padding_frames': 30,\n",
       " 'result_folder': 'data/MEVA/meva_processed',\n",
       " 'split_seed': 42,\n",
       " 'target_activities': ['person_talks_on_phone',\n",
       "  'person_texts_on_phone',\n",
       "  'person_picks_up_object',\n",
       "  'person_reads_document',\n",
       "  'person_interacts_with_laptop'],\n",
       " 'test_size': 0.2,\n",
       " 'videos_root': 'data/MEVA/mevadata-public-01/drops-123-r13'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_source = YamlConfigReader(conf_file_path)\n",
    "params = config_source.get_all()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person_talks_on_phone': 0,\n",
       " 'person_texts_on_phone': 1,\n",
       " 'person_picks_up_object': 2,\n",
       " 'person_reads_document': 3,\n",
       " 'person_interacts_with_laptop': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2idx = {class_name: i for i, class_name in enumerate(params['target_activities'])}\n",
    "idx2class = {v: k for k, v in class2idx.items()}\n",
    "class2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = dict()\n",
    "image_datasets['test'] = RandomDatasetDecord(\n",
    "    data_folder_path=params['result_folder'],\n",
    "    csv_file_path=params['test'],\n",
    "    class2idx=class2idx,\n",
    "    video_transform=None,\n",
    "    epoch_size_ratio=1.0,\n",
    "    frame_sample_rate=1,\n",
    "    clip_len=16,\n",
    "    num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_path='2018-03-15/15/2018-03-15.15-40-00.15-45-00.school.G421_frange0-9001.avi'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, x in image_datasets['test']:\n",
    "    if i == 1: break\n",
    "    print(x['video'].shape)\n",
    "    print(x['target'])\n",
    "    print(x['path'])\n",
    "    print(x['bbox'])\n",
    "    print()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2025\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "dataloaders = dict()\n",
    "dataloaders['test'] = DataLoader(image_datasets['test'],\n",
    "                                 batch_size=batch_size,\n",
    "                                 pin_memory=True,\n",
    "                                 collate_fn=collate_fn,\n",
    "                                 worker_init_fn=seed_worker,\n",
    "                                 generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_progress = tqdm(enumerate(dataloaders['test']), \n",
    "                     total=len(dataloaders['test']))\n",
    "\n",
    "for i, batch in tqdm_progress:\n",
    "    if i == 0:\n",
    "        print(f\"{batch['path']=}\\n{batch['video'].shape=}\")\n",
    "        first_video = batch['video'][0].permute(1, 0, 2, 3)\n",
    "        print(f\"{first_video.shape=}\")\n",
    "        print(f\"pixel range: {first_video.min()} - {first_video.max()}\")\n",
    "\n",
    "        print(f\"{len(batch['bbox'])=}\")\n",
    "        bbox_coords = batch['bbox'][0].bbox.tolist()\n",
    "        print(bbox_coords)\n",
    "\n",
    "        print(f\"{len(batch['target'])=}\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, clip_len//2, figsize=(12, 12))\n",
    "\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            img = first_video[i].permute(1, 2, 0).numpy()\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            for bbox_coord in bbox_coords:\n",
    "                x_min, y_min, x_max, y_max = bbox_coord\n",
    "                width = x_max - x_min\n",
    "                height = y_max - y_min\n",
    "                rect = patches.Rectangle((x_min, y_min), width, height,\n",
    "                                          linewidth=2, edgecolor='red', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    break\n",
    "\n",
    "print(f\"Elapsed: {tqdm_progress.format_dict['elapsed']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
